---
title: Spatio-Temporal Encoding of Brain Dynamics with Surface Masked Autoencoders
abstract: The development of robust and generalisable models for encoding the spatio-temporal
  dynamics of human brain activity is crucial for advancing neuroscientific discoveries.
  However, significant individual variation in the organisation of the human cerebral
  cortex makes it difficult to identify population-level trends in these signals.
  Recently, Surface Vision Transformers (SiTs) have emerged as a promising approach
  for modelling cortical signals, yet they face some limitations in low-data scenarios
  due to the lack of inductive biases in their architecture. To address these challenges,
  this paper proposes the surface Masked AutoEncoder (sMAE) and video surface Masked
  AutoEncoder (vsMAE) - for multivariate and spatio-temporal pre-training of cortical
  signals over regular icosahedral grids. These models are trained to reconstruct
  cortical feature maps from masked versions of the input by learning strong latent
  representations of cortical structure and function. Such representations translate
  into better modelling of individual phenotypes and enhanced performance in downstream
  tasks. The proposed approach was evaluated on cortical phenotype regression using
  data from the young adult Human Connectome Project (HCP) and developing HCP (dHCP).
  Results show that (v)sMAE pre-trained models improve phenotyping prediction performance
  on multiple tasks by $\ge 26%$, and offer faster convergence relative to models
  trained from scratch.  Finally, we show that pre-training vision transformers on
  large datasets, such as the UK Biobank (UKB), supports transfer learning to low-data
  regimes. Our code and pre-trained models are publicly available at https://github.com/metrics-lab/surface-masked-autoencoders.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dahan24b
month: 0
tex_title: Spatio-Temporal Encoding of Brain Dynamics with Surface Masked Autoencoders
firstpage: 306
lastpage: 325
page: 306-325
order: 306
cycles: false
bibtex_author: Dahan, Simon and Williams, Logan Zane John and Guo, Yourong and Rueckert,
  Daniel and Robinson, Emma Claire
author:
- given: Simon
  family: Dahan
- given: Logan Zane John
  family: Williams
- given: Yourong
  family: Guo
- given: Daniel
  family: Rueckert
- given: Emma Claire
  family: Robinson
date: 2024-12-23
address:
container-title: Proceedings of The 7nd International Conference on Medical Imaging
  with Deep Learning
volume: '250'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v250/main/assets/dahan24b/dahan24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
