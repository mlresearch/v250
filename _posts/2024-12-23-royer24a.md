---
title: 'MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language
  Models'
abstract: We introduce MultiMedEval, an open-source toolkit for fair and reproducible
  evaluation of large, medical vision-language models (VLM). MultiMedEval comprehensively
  assesses the models’ performance on a broad array of six multi-modal tasks, conducted
  over 23 datasets, and spanning over 11 medical domains. The chosen tasks and performance
  metrics are based on their widespread adoption in the community and their diversity,
  ensuring a thorough evaluation of the model’s overall generalizability. We open-source
  a Python toolkit (https://anonymous.4open.science/r/MultiMedEval-C780) with a simple
  interface and setup process, enabling the evaluation of any VLM in just a few lines
  of code. Our goal is to simplify the intricate landscape of VLM evaluation, thus
  promoting fair and uniform benchmarking of future VLMs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: royer24a
month: 0
tex_title: 'MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language
  Models'
firstpage: 1310
lastpage: 1327
page: 1310-1327
order: 1310
cycles: false
bibtex_author: Royer, Corentin and Menze, Bjoern and Sekuboyina, Anjany
author:
- given: Corentin
  family: Royer
- given: Bjoern
  family: Menze
- given: Anjany
  family: Sekuboyina
date: 2024-12-23
address:
container-title: Proceedings of The 7nd International Conference on Medical Imaging
  with Deep Learning
volume: '250'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v250/main/assets/royer24a/royer24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
