---
title: 'VariViT: A Vision Transformer for Variable Image Sizes'
abstract: Vision Transformers (ViTs) have emerged as the state-of-the-art architecture
  in representation learning, leveraging self-attention mechanisms to excel in various
  tasks. ViTs split images into fixed-size patches, constraining them to a predefined
  size and necessitating pre-processing steps like resizing, padding, or cropping.
  This poses challenges in medical imaging, particularly with irregularly shaped structures
  like tumors. A fixed bounding box crop size produces input images with highly variable
  foreground-to-background ratios. Resizing medical images can degrade information
  and introduce artifacts, impacting diagnosis. Hence, tailoring variable-sized crops
  to regions of interest can enhance feature representation capabilities. Moreover,
  large images are computationally expensive, and smaller sizes risk information loss,
  presenting a computation-accuracy tradeoff. We propose VariViT, an improved ViT
  model crafted to handle variable image sizes while maintaining a consistent patch
  size. VariViT employs a novel positional embedding resizing scheme for a variable
  number of patches. We also implement a new batching strategy within VariViT to reduce
  computational complexity, resulting in faster training and inference times. In our
  evaluations on two 3D brain MRI datasets, VariViT surpasses vanilla ViTs and ResNet
  in glioma genotype prediction and brain tumor classification. It achieves F1-scores
  of 75.5% and 76.3%, respectively, learning more discriminative features. Our proposed
  batching strategy reduces computation time by up to 30% compared to conventional
  architectures. These findings underscore the efficacy of VariViT in image representation
  learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: varma24a
month: 0
tex_title: 'VariViT: A Vision Transformer for Variable Image Sizes'
firstpage: 1571
lastpage: 1583
page: 1571-1583
order: 1571
cycles: false
bibtex_author: Varma, Aswathi and Shit, Suprosanna and Prabhakar, Chinmay and Scholz,
  Daniel and Li, Hongwei Bran and Menze, Bjoern and Rueckert, Daniel and Wiestler,
  Benedikt
author:
- given: Aswathi
  family: Varma
- given: Suprosanna
  family: Shit
- given: Chinmay
  family: Prabhakar
- given: Daniel
  family: Scholz
- given: Hongwei Bran
  family: Li
- given: Bjoern
  family: Menze
- given: Daniel
  family: Rueckert
- given: Benedikt
  family: Wiestler
date: 2024-12-23
address:
container-title: Proceedings of The 7nd International Conference on Medical Imaging
  with Deep Learning
volume: '250'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v250/main/assets/varma24a/varma24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
