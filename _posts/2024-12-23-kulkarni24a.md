---
title: 'Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable
  Patient Populations'
abstract: The proliferation of artificial intelligence (AI) in radiology has shed
  light on the risk of deep learning (DL) models exacerbating clinical biases towards
  vulnerable patient populations. While prior literature has focused on quantifying
  biases exhibited by trained DL models, demographically targeted adversarial bias
  attacks on DL models and its implication in the clinical environment remains an
  underexplored field of research in medical imaging. In this work, we demonstrate
  that demographically targeted label poisoning attacks can introduce undetectable
  underdiagnosis bias in DL models. Our results across multiple performance metrics
  and demographic groups like sex, age, and their intersectional subgroups show that
  adversarial bias attacks demonstrate high-selectivity for bias in the targeted group
  by degrading group model performance without impacting overall model performance.
  Furthermore, our results indicate that adversarial bias attacks result in biased
  DL models that propagate prediction bias even when evaluated with external datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kulkarni24a
month: 0
tex_title: 'Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable
  Patient Populations'
firstpage: 793
lastpage: 821
page: 793-821
order: 793
cycles: false
bibtex_author: Kulkarni, Pranav and Chan, Andrew and Navarathna, Nithya and Chan,
  Skylar and Yi, Paul and Parekh, Vishwa Sanjay
author:
- given: Pranav
  family: Kulkarni
- given: Andrew
  family: Chan
- given: Nithya
  family: Navarathna
- given: Skylar
  family: Chan
- given: Paul
  family: Yi
- given: Vishwa Sanjay
  family: Parekh
date: 2024-12-23
address:
container-title: Proceedings of The 7nd International Conference on Medical Imaging
  with Deep Learning
volume: '250'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 12
  - 23
pdf: https://raw.githubusercontent.com/mlresearch/v250/main/assets/kulkarni24a/kulkarni24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
